{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquiring raw texts for Finger-tweets and Finger-news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter API and the Finnish Wikipedia are queried to fetch the raw texts that make up Finger-tweets and Finger-news, respectively. The outputs of these queries are used when annotating in Label-studio to create the actual corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finger-tweets\n",
    "I use [Twarc](https://twarc-project.readthedocs.io/en/latest/) for this. Please note that you need a [Twitter API key](https://developer.twitter.com/en/docs/twitter-api/getting-started/getting-access-to-the-twitter-api) and [configure Twarc](https://twarc-project.readthedocs.io/en/latest/twarc2_en_us/#configure) to successfully query the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twarc import Twarc\n",
    "# initializing Twarc\n",
    "t = Twarc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The block below will run a query on twitter search API under these conditions:\n",
    "1. The tweet contains the conjunction \"ja\" / \"and\". This is used because the query cannot be empty and I therefore decided to use a stopword\n",
    "2. Twitter has identified the tweet to be in Finnish. The identification may fail especially for short tweets, but it's good enough\n",
    "\n",
    "The output is saved to as JSON lines into whatever output path you pass as a parameter (the section after >)\n",
    "\n",
    "Please note that the query will return tens of thousands of tweets and will run for quite a while. It's also quite sticky, can't seem to cancel that easily. Interrupting the kernel should work, but if nothing else works, just close the Jupyter notebook session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!twarc search 'ja' --lang fi > input_data/finnish_tweet_search.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the JSON back as a Pandas dataframe\n",
    "Filtering and sampling will be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_json(\"input_data/finnish_tweet_search.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure the tweet is an original one and not a \n",
    "1. Retweet\n",
    "2. Quote retweet\n",
    "3. A reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = tweets[(pd.isnull(tweets['retweeted_status'])) \n",
    "                  & (pd.isnull(tweets['quoted_status']) \n",
    "                     & (pd.isnull(tweets['in_reply_to_screen_name']))\n",
    "                     & (tweets['lang'] =='fi'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length is: 56700 \n",
      "Filtered length is: 8915\n"
     ]
    }
   ],
   "source": [
    "print(\"Original length is:\", len(tweets), \"\\nFiltered length is:\", len(filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['created_at', 'id', 'id_str', 'full_text', 'truncated',\n",
       "       'display_text_range', 'entities', 'metadata', 'source',\n",
       "       'in_reply_to_status_id', 'in_reply_to_status_id_str',\n",
       "       'in_reply_to_user_id', 'in_reply_to_user_id_str',\n",
       "       'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place',\n",
       "       'contributors', 'retweeted_status', 'is_quote_status', 'retweet_count',\n",
       "       'favorite_count', 'favorited', 'retweeted', 'lang',\n",
       "       'possibly_sensitive', 'extended_entities', 'quoted_status_id',\n",
       "       'quoted_status_id_str', 'quoted_status'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#there're a ton of columns that are useless for the corpus use case\n",
    "tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep Tweet identifier and text\n",
    "output_df = filtered[['id_str', 'full_text']]\n",
    "# randomly sample a thousand tweets\n",
    "output_df = output_df.sample(n=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_csv(\"input_data/filtered_finnish_tweets_sample1000.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finger-news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have collected a list of URL's by hand (\"wikiuutiset_2011_urls.txt\"). For each of the urls, UrlLib is used to get the HTML representation of that page, which is then queried with [BeatifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/) to get article titles and texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_url_list = 'input_data/wikiuutiset_2011_urls.txt'\n",
    "with open(path_to_url_list) as f:\n",
    "    urls = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikinews_article(url):\n",
    "    # read source, get it to Beatifulsoup object\n",
    "    source = urlopen(url).read()\n",
    "    soup = BeautifulSoup(source,'lxml')\n",
    "    \n",
    "    # get title, add it to a text string w/ line change\n",
    "    title = soup.find('title').text\n",
    "    text = title +'\\n'\n",
    "    for paragraph in soup.find_all('p'):\n",
    "        # append each parahraph to the text string\n",
    "        text += paragraph.text\n",
    "        \n",
    "        # this regular expression removes citations [1] etc. from the text\n",
    "        text = re.sub(r'\\[.*?\\]+', '', text)\n",
    "        \n",
    "    return title, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, below is one article text and title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall Streetin miljardööri Raj Rajaratnam syyllistyi sisäpiirikauppaan – Wikiuutiset\n",
      "Wall Streetin miljardööri Raj Rajaratnam syyllistyi sisäpiirikauppaan – Wikiuutiset\n",
      "11. toukokuuta 2011\n",
      " Wall Streetin sijoittaja  Raj Rajaratnam tuomittiin syylliseksi laittomista sisäpiirikaupoista.  Srilankalainen miljardööri hyötyi syyttäjien mukaan 60 miljoonaa dollaria eli yli 40 miljoonaa euroa laittomista sisäpiirivihjeistä.\n",
      " Wikipedian mukaan FBI pidätti hänet 16. lokakuuta 2009 epäiltynä sisäpiirikaukoista. Hallitus on tutkinut tapausta vuodesta 2008. Rajaratnam sai laittomia vihjeitä yrityksiltä kuten  Goldman Sachs. Syyttäjät nauhottivat salaa Rajaratnamin puheluita. Nauhoilla vihjeitä antoi mm. Goldman Sachsin johtaja   Rajat Gupta. Tuomiosta voi valittaa.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "title, text = get_wikinews_article(urls[15])\n",
    "print(title)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looping through the texts, acquiring titles and texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "texts = []\n",
    "\n",
    "for url in urls:\n",
    "    title, text = get_wikinews_article(url)\n",
    "    titles.append(title)\n",
    "    texts.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Casting the inputs to a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikinews_articles = pd.DataFrame({'title':titles,'text':texts,'url':urls})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vuodenvaihteen jälkeen 336 kuntaa – Wikiuutiset</td>\n",
       "      <td>Vuodenvaihteen jälkeen 336 kuntaa – Wikiuutise...</td>\n",
       "      <td>https://fi.wikinews.org/wiki/Vuodenvaihteen_j%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kolavian Tupolev Tu-154 -kone tuhoutui tulipal...</td>\n",
       "      <td>Kolavian Tupolev Tu-154 -kone tuhoutui tulipal...</td>\n",
       "      <td>https://fi.wikinews.org/wiki/Kolavian_Tupolev_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0    Vuodenvaihteen jälkeen 336 kuntaa – Wikiuutiset   \n",
       "1  Kolavian Tupolev Tu-154 -kone tuhoutui tulipal...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Vuodenvaihteen jälkeen 336 kuntaa – Wikiuutise...   \n",
       "1  Kolavian Tupolev Tu-154 -kone tuhoutui tulipal...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://fi.wikinews.org/wiki/Vuodenvaihteen_j%...  \n",
       "1  https://fi.wikinews.org/wiki/Kolavian_Tupolev_...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikinews_articles.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikinews_articles.to_csv('input_data/wikinews_2011.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
